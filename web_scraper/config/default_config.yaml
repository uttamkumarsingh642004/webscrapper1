# Default Web Scraper Configuration

# Scraping Settings
scraping:
  # Scraper type: auto, static, selenium, playwright, api
  scraper_type: "auto"

  # Timeout settings (seconds)
  timeout: 30
  page_load_timeout: 60

  # Retry settings
  max_retries: 3
  retry_delay: 1  # seconds
  backoff_factor: 2  # exponential backoff multiplier

  # Rate limiting
  rate_limit: 1  # requests per second
  delay_between_requests: 1  # seconds

  # Concurrency
  max_workers: 5
  enable_async: false

  # Browser settings (for Selenium/Playwright)
  headless: true
  browser: "chrome"  # chrome, firefox, edge

  # Pagination
  max_pages: 10
  pagination_type: "auto"  # auto, numbered, infinite_scroll, load_more

# Request Settings
request:
  # Headers
  headers:
    Accept: "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"
    Accept-Language: "en-US,en;q=0.9"
    Accept-Encoding: "gzip, deflate, br"
    DNT: "1"

  # User Agent rotation
  rotate_user_agent: true
  custom_user_agents: []

  # Proxy settings
  use_proxy: false
  proxy_file: null
  rotate_proxy: true

  # Session management
  maintain_session: true
  cookies_file: null

  # Authentication
  auth:
    enabled: false
    type: "basic"  # basic, bearer, custom
    username: null
    password: null
    token: null

# Extraction Settings
extraction:
  # Selectors
  css_selectors: {}
  xpath_selectors: {}
  regex_patterns: {}

  # Data cleaning
  clean_whitespace: true
  remove_html_tags: true
  normalize_unicode: true

  # Duplicate handling
  remove_duplicates: true
  duplicate_key: "url"

# Export Settings
export:
  # Output format: json, csv, excel, sqlite, mongodb, all
  format: "json"

  # Output file path
  output_file: "scraped_data.json"

  # JSON settings
  json_indent: 2
  json_ensure_ascii: false

  # CSV settings
  csv_delimiter: ","
  csv_quoting: "minimal"

  # Database settings
  database:
    sqlite_file: "scraped_data.db"
    table_name: "scraped_items"
    mongodb_uri: "mongodb://localhost:27017/"
    mongodb_database: "web_scraper"
    mongodb_collection: "scraped_items"

  # Data validation
  validate_schema: false
  schema_file: null

  # Incremental scraping
  incremental: false
  checkpoint_file: ".scraper_checkpoint.json"

# Error Handling
error_handling:
  # Logging
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  log_file: "scraper.log"
  log_to_console: true
  log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

  # Error recovery
  continue_on_error: true
  save_failed_urls: true
  failed_urls_file: "failed_urls.txt"

  # Notifications
  notify_on_captcha: true
  notify_on_error: false

# Advanced Features
advanced:
  # Robots.txt compliance
  respect_robots_txt: true

  # Caching
  enable_cache: false
  cache_dir: ".scraper_cache"
  cache_expiry: 3600  # seconds

  # Screenshots
  take_screenshots: false
  screenshot_dir: "screenshots"
  screenshot_on_error: false

  # Monitoring
  track_bandwidth: true
  show_progress_bar: true

  # AJAX detection
  detect_ajax: true
  wait_for_ajax: true
  ajax_wait_time: 5  # seconds

  # Form handling
  enable_form_submission: false

# Scheduling (optional)
scheduling:
  enabled: false
  cron_expression: null  # e.g., "0 */6 * * *" for every 6 hours
  timezone: "UTC"
